{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1789a98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import pickle\n",
    "import csv\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ea0d40",
   "metadata": {},
   "source": [
    "### Auxiliary functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dd1d3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    '''\n",
    "    Load train, valid and test dataset in a specified year.\n",
    "\n",
    "    Returns:\n",
    "    -----------------------------------\n",
    "    (train_data, valid_data, test_data): a tuple of train, valid, test dataset. Each dataset is represented by a numpy array where each row is a trip sample (src, dst, count).\n",
    "    '''\n",
    "    # load train data\n",
    "    train = pd.read_csv('data/LODES/CommutingFlow_2015_train_Denver.csv')\n",
    "    # load valid data\n",
    "    valid = pd.read_csv('data/LODES/CommutingFlow_2015_valid_Denver.csv')\n",
    "    # load test data\n",
    "    test = pd.read_csv('data/LODES/CommutingFlow_2015_test_Denver.csv')\n",
    "    # mapping BoroCT2010 to node id\n",
    "    mapping_table = pd.read_csv('data/CensusTract/mapping_NodeIDDenverCT.csv')\n",
    "    train = ct2nid(train, mapping_table)\n",
    "    valid = ct2nid(valid, mapping_table)\n",
    "    test = ct2nid(test, mapping_table)\n",
    "    # construct in/out flow count for training\n",
    "    inflow_train = pd.DataFrame(index=mapping_table['node_id']) # container\n",
    "    outflow_train = pd.DataFrame(index=mapping_table['node_id']) # container\n",
    "    inflow = train.groupby('dst').agg({'count': 'sum'}) # stats\n",
    "    outflow = train.groupby('src').agg({'count': 'sum'}) # stats\n",
    "    inflow.index.name = 'node_id'\n",
    "    outflow.index.name = 'node_id'\n",
    "    inflow_train['count'] = inflow # pass the value\n",
    "    outflow_train['count'] = outflow # pass the value\n",
    "    inflow_train = inflow_train.fillna(0).sort_index() # fillna\n",
    "    outflow_train = outflow_train.fillna(0).sort_index() # fillna\n",
    "    # load node feature table\n",
    "    node_feats = pd.read_csv('data/PLUTO/denver_ct_ratios2015.csv')\n",
    "    node_feats['CT'] = mapping_table.set_index('CT').loc[node_feats['CT']].values # map census tract to node id\n",
    "    node_feats = node_feats.rename(columns={'CT': 'nid'}).set_index('nid').sort_index()\n",
    "    # sanity check\n",
    "    if node_feats.isnull().values.any(): # if there is any NaN in nodes' feature table\n",
    "        node_feats.fillna(0, inplace=True)\n",
    "        warnings.warn('Feature table contains NaN. 0 is used to fill these NaNs')\n",
    "    # normalization\n",
    "    node_feats = (node_feats - node_feats.mean()) / node_feats.std()\n",
    "    # load adjacency matrix\n",
    "    ct_adj = pd.read_csv('data/CensusTract/adjacency_matrix_withweightDenver.csv', index_col=0)\n",
    "    ct_inorder = mapping_table.sort_values(by='node_id')['CT']\n",
    "    ct_adj = ct_adj.loc[ct_inorder, ct_inorder.astype(str)]\n",
    "    # min-max scale the weights\n",
    "    ct_adj = ct_adj / ct_adj.max().max() # min is 0\n",
    "    # fill nan with 0\n",
    "    ct_adj = ct_adj.fillna(0)\n",
    "    # load distance matrix\n",
    "    distm = pd.read_csv('data/OSRM/census_tract_trip_duration_matrix_Denver.csv', index_col='CT')\n",
    "    # define column order\n",
    "    cols = ['src', 'dst', 'count']\n",
    "    # return\n",
    "    data = {}\n",
    "    data['train'] = train[cols].values\n",
    "    data['valid'] = valid[cols].values\n",
    "    data['test'] = test[cols].values\n",
    "    data['train_inflow'] = inflow_train.values\n",
    "    data['train_outflow'] = outflow_train.values\n",
    "    data['num_nodes'] = ct_adj.shape[0]\n",
    "    data['node_feats'] = node_feats.values\n",
    "    data['ct_adjacency_withweight'] = ct_adj.values\n",
    "    data['distm'] = distm.values\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd405631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ct2nid(dataframe, mapping_table):\n",
    "    '''\n",
    "    Mapping BoroCT2010 code to node id.\n",
    "\n",
    "    Inputs: \n",
    "    -----------------------------------\n",
    "    dataframe: The dataframe of trips. it is supposed to have 3 columns: h_BoroCT2010, w_BoroCT2010, count\n",
    "    mapping_table: The table of mapping between node id and BoroCT2010. The table is supposed to have two columns: node_id, BoroCT2010\n",
    "\n",
    "    Returns:\n",
    "    -----------------------------------\n",
    "    frame: it is supposed to have 3 columns: src, dst, count. src and dst are the node id of source and target respectively.\n",
    "    '''\n",
    "    frame = dataframe.copy()\n",
    "    mapping = mapping_table.copy()\n",
    "    # do the mapping\n",
    "    mapping = mapping.set_index('CT')\n",
    "    frame['src'] = mapping.loc[frame['h_geocode']].values\n",
    "    frame['dst'] = mapping.loc[frame['w_geocode']].values\n",
    "    # return\n",
    "    return frame[['src', 'dst', 'count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cec3197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_feat_from_srcemb_dstemb_dist(triplets, src_emb, dst_emb, dist):\n",
    "    feat_src = src_emb[triplets[:, 0]]\n",
    "    feat_dst = dst_emb[triplets[:, 1]]\n",
    "    feat_dist = dist[triplets[:, 0], triplets[:, 1]].reshape(-1, 1)\n",
    "    X = np.concatenate([feat_src, feat_dst, feat_dist], axis=1)\n",
    "    y = triplets[:, 2]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1512f7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(y_hat, y):\n",
    "    '''\n",
    "    Root Mean Square Error Metric\n",
    "    '''\n",
    "    return np.sqrt(np.mean((y_hat - y)**2))\n",
    "\n",
    "def CPC(y_hat, y):\n",
    "    '''\n",
    "    Common Part of Commuters Metric\n",
    "    '''\n",
    "    common = np.min(np.stack((y_hat, y), axis=1), axis=1)\n",
    "    return 2 * np.sum(common) / (np.sum(y_hat) + np.sum(y))\n",
    "\n",
    "def CPL(y_hat, y):\n",
    "    '''\n",
    "    Common Part of Links Metric. \n",
    "    \n",
    "    Check the topology.\n",
    "    '''\n",
    "    yy_hat = y_hat > 0\n",
    "    yy = y > 0\n",
    "    return 2 * np.sum(yy_hat * yy) / (np.sum(yy_hat) + np.sum(yy))\n",
    "\n",
    "def MAPE(y_hat, y):\n",
    "    '''\n",
    "    Mean Absolute Percentage Error Metric\n",
    "    '''\n",
    "    abserror = np.abs(y_hat - y)\n",
    "    return np.mean(abserror / y)\n",
    "\n",
    "def MAE(y_hat, y):\n",
    "    '''\n",
    "    Mean Absolute Error Metric\n",
    "    '''\n",
    "    abserror = np.abs(y_hat - y)\n",
    "    return np.mean(abserror)\n",
    "\n",
    "def NRMSE(RMSE, y):\n",
    "    '''\n",
    "    Normalized Root Mean Square Error Metric\n",
    "    '''\n",
    "    return RMSE/(y.max() - y.min())\n",
    "\n",
    "def evaluate(y_hat, y):\n",
    "    '''\n",
    "    Evaluate the error in different metrics\n",
    "    '''\n",
    "    # metric\n",
    "    rmse = RMSE(y_hat, y)\n",
    "    mae = MAE(y_hat, y)\n",
    "    mape = MAPE(y_hat, y)\n",
    "    cpc = CPC(y_hat, y)\n",
    "    cpl = CPL(y_hat, y)\n",
    "    nrmse = NRMSE(rmse,y)\n",
    "    # return\n",
    "    return {'RMSE': rmse, 'MAE': mae, 'MAPE': mape, 'CPC': cpc, 'CPL': cpl, 'NRMSE':nrmse}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb832f63",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe3b1241",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"mahalanobis\"\n",
    "\n",
    "# \"manhattan\"\n",
    "# \"euclidean\"\n",
    "# \"chebyshev\"\n",
    "# \"minkowski\"\n",
    "# \"wminkowski\"\n",
    "# \"seuclidean\"\n",
    "# \"mahalanobis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4499251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading files\n",
    "cityA_features = pd.read_csv('data/PLUTO/columbus_ct_ratios2015.csv')\n",
    "cityB_features = pd.read_csv('data/PLUTO/denver_ct_ratios2015.csv')\n",
    "\n",
    "cityA_features = cityA_features[cityB_features.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b589d27",
   "metadata": {},
   "source": [
    "#### Find nearest neighbors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e95897fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the nearest neighbors\n",
    "X_train = cityA_features.drop(columns=['CT'])\n",
    "y_train = cityA_features['CT']\n",
    "\n",
    "knn = NearestNeighbors(metric=metric, metric_params={'V': np.cov(X_train, rowvar=False)})\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "X_test = cityB_features.drop(columns=['CT'])\n",
    "y_test = cityB_features['CT']\n",
    "\n",
    "list_ct, list_neighbors = [], []\n",
    "for ix in cityB_features.index:\n",
    "  list_ct.append(y_test[ix])\n",
    "  list_neighbors.append(knn.kneighbors(X_test.iloc[ix,:].values.reshape(1, -1))[1])\n",
    "\n",
    "data = {'ct_cityB': list_ct, 'neighbors_cityA': list_neighbors}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893ef337",
   "metadata": {},
   "source": [
    "#### Preprocess dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "122373bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data = load_dataset()\n",
    "# parse dataset\n",
    "train_data = data['train']\n",
    "valid_data = data['valid']\n",
    "test_data = data['test']\n",
    "distm = data['distm']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d8afb6",
   "metadata": {},
   "source": [
    "#### Create new src and dst embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1520079d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings\n",
    "epath = 'embeddings/censustract_embeddings_year2015_layers1_emb128_multitask(0.5, 0.25, 0.25).npz'\n",
    "embeddings = np.load(epath, allow_pickle=True)\n",
    "# parse embeddings\n",
    "src_emb, dst_emb = embeddings['arr_0'], embeddings['arr_1']\n",
    "# scale distance matrix\n",
    "scaled_distm = distm / distm.max() * np.max([src_emb.max(), dst_emb.max()])\n",
    "# construct src and dst embeddings to city B from city A\n",
    "src_emb_new, dst_emb_new = [], []\n",
    "\n",
    "for neigh in df['neighbors_cityA']:\n",
    "    src_emb_new.append(src_emb[neigh[0]].mean(axis=0))\n",
    "    dst_emb_new.append(dst_emb[neigh[0]].mean(axis=0))\n",
    "\n",
    "src_emb_new = np.array(src_emb_new)\n",
    "src_emb_new = np.array(src_emb_new)\n",
    "\n",
    "# construct sample features\n",
    "X_train, y_train = construct_feat_from_srcemb_dstemb_dist(train_data, src_emb_new, src_emb_new, scaled_distm)\n",
    "X_valid, y_valid = construct_feat_from_srcemb_dstemb_dist(valid_data, src_emb_new, src_emb_new, scaled_distm)\n",
    "X_test, y_test = construct_feat_from_srcemb_dstemb_dist(test_data, src_emb_new, src_emb_new, scaled_distm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0966d0",
   "metadata": {},
   "source": [
    "#### Training model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "171af1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train gbrt\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, random_state=2019, n_estimators=100)\n",
    "gbrt.fit(X_train, y_train)\n",
    "# test\n",
    "y_gbrt = gbrt.predict(X_test)\n",
    "res = evaluate(y_gbrt, y_test)\n",
    "# save result\n",
    "with open('outputs/overall_performance.csv', 'a+') as fout:\n",
    "    # create writer\n",
    "    cols = ['metric', 'model','RMSE', 'MAE', 'MAPE', 'CPC', 'CPL','NRMSE']\n",
    "    writer = csv.DictWriter(fout, fieldnames=cols)\n",
    "    # create model name\n",
    "    model_name = 'GMEL_year2019_layers1_emb128_multitask(0.5, 0.25, 0.25)'\n",
    "    res['metric'] = metric\n",
    "    res['model'] = model_name\n",
    "    # write result\n",
    "    writer.writerow(res)\n",
    "# # save model\n",
    "# with open('models/gbrt_year2015_layers1_emb128_multitask(0.5, 0.25, 0.25).txt', 'a+') as fout:\n",
    "#     pickle.dump(gbrt, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "62160a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>CPC</th>\n",
       "      <th>CPL</th>\n",
       "      <th>NRMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>euclidean</th>\n",
       "      <td>GMEL_year2019_layers1_emb128_multitask(0.5, 0....</td>\n",
       "      <td>8.648222</td>\n",
       "      <td>4.451674</td>\n",
       "      <td>1.305319</td>\n",
       "      <td>0.683602</td>\n",
       "      <td>0.999820</td>\n",
       "      <td>0.058831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manhattan</th>\n",
       "      <td>GMEL_year2019_layers1_emb128_multitask(0.5, 0....</td>\n",
       "      <td>8.691705</td>\n",
       "      <td>4.462365</td>\n",
       "      <td>1.337679</td>\n",
       "      <td>0.683273</td>\n",
       "      <td>0.999820</td>\n",
       "      <td>0.059127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chebyshev</th>\n",
       "      <td>GMEL_year2019_layers1_emb128_multitask(0.5, 0....</td>\n",
       "      <td>8.936493</td>\n",
       "      <td>4.550395</td>\n",
       "      <td>1.356366</td>\n",
       "      <td>0.676507</td>\n",
       "      <td>0.999461</td>\n",
       "      <td>0.060792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minkowski</th>\n",
       "      <td>GMEL_year2019_layers1_emb128_multitask(0.5, 0....</td>\n",
       "      <td>8.648222</td>\n",
       "      <td>4.451674</td>\n",
       "      <td>1.305319</td>\n",
       "      <td>0.683602</td>\n",
       "      <td>0.999820</td>\n",
       "      <td>0.058831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mahalanobis</th>\n",
       "      <td>GMEL_year2019_layers1_emb128_multitask(0.5, 0....</td>\n",
       "      <td>10.145984</td>\n",
       "      <td>5.445827</td>\n",
       "      <td>1.756627</td>\n",
       "      <td>0.613205</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         model       RMSE  \\\n",
       "euclidean    GMEL_year2019_layers1_emb128_multitask(0.5, 0....   8.648222   \n",
       "manhattan    GMEL_year2019_layers1_emb128_multitask(0.5, 0....   8.691705   \n",
       "chebyshev    GMEL_year2019_layers1_emb128_multitask(0.5, 0....   8.936493   \n",
       "minkowski    GMEL_year2019_layers1_emb128_multitask(0.5, 0....   8.648222   \n",
       "mahalanobis  GMEL_year2019_layers1_emb128_multitask(0.5, 0....  10.145984   \n",
       "\n",
       "                  MAE      MAPE       CPC       CPL     NRMSE  \n",
       "euclidean    4.451674  1.305319  0.683602  0.999820  0.058831  \n",
       "manhattan    4.462365  1.337679  0.683273  0.999820  0.059127  \n",
       "chebyshev    4.550395  1.356366  0.676507  0.999461  0.060792  \n",
       "minkowski    4.451674  1.305319  0.683602  0.999820  0.058831  \n",
       "mahalanobis  5.445827  1.756627  0.613205  1.000000  0.069020  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv('outputs/overall_performance.csv', names=['model','RMSE', 'MAE', 'MAPE', 'CPC', 'CPL','NRMSE'])\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc19dde1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
