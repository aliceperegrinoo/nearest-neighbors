{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1789a98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import pickle\n",
    "import csv\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ea0d40",
   "metadata": {},
   "source": [
    "### Auxiliary functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7dd1d3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    '''\n",
    "    Load train, valid and test dataset in a specified year.\n",
    "\n",
    "    Returns:\n",
    "    -----------------------------------\n",
    "    (train_data, valid_data, test_data): a tuple of train, valid, test dataset. Each dataset is represented by a numpy array where each row is a trip sample (src, dst, count).\n",
    "    '''\n",
    "    # load train data\n",
    "    train = pd.read_csv('data/LODES/CommutingFlow_2015_train_Denver.csv')\n",
    "    # load valid data\n",
    "    valid = pd.read_csv('data/LODES/CommutingFlow_2015_valid_Denver.csv')\n",
    "    # load test data\n",
    "    test = pd.read_csv('data/LODES/CommutingFlow_2015_test_Denver.csv')\n",
    "    # mapping BoroCT2010 to node id\n",
    "    mapping_table = pd.read_csv('data/CensusTract/mapping_NodeIDDenverCT.csv')\n",
    "    train = ct2nid(train, mapping_table)\n",
    "    valid = ct2nid(valid, mapping_table)\n",
    "    test = ct2nid(test, mapping_table)\n",
    "    # construct in/out flow count for training\n",
    "    inflow_train = pd.DataFrame(index=mapping_table['node_id']) # container\n",
    "    outflow_train = pd.DataFrame(index=mapping_table['node_id']) # container\n",
    "    inflow = train.groupby('dst').agg({'count': 'sum'}) # stats\n",
    "    outflow = train.groupby('src').agg({'count': 'sum'}) # stats\n",
    "    inflow.index.name = 'node_id'\n",
    "    outflow.index.name = 'node_id'\n",
    "    inflow_train['count'] = inflow # pass the value\n",
    "    outflow_train['count'] = outflow # pass the value\n",
    "    inflow_train = inflow_train.fillna(0).sort_index() # fillna\n",
    "    outflow_train = outflow_train.fillna(0).sort_index() # fillna\n",
    "    # load node feature table\n",
    "    node_feats = pd.read_csv('data/PLUTO/denver_ct_ratios2015.csv')\n",
    "    node_feats['CT'] = mapping_table.set_index('CT').loc[node_feats['CT']].values # map census tract to node id\n",
    "    node_feats = node_feats.rename(columns={'CT': 'nid'}).set_index('nid').sort_index()\n",
    "    # sanity check\n",
    "    if node_feats.isnull().values.any(): # if there is any NaN in nodes' feature table\n",
    "        node_feats.fillna(0, inplace=True)\n",
    "        warnings.warn('Feature table contains NaN. 0 is used to fill these NaNs')\n",
    "    # normalization\n",
    "    node_feats = (node_feats - node_feats.mean()) / node_feats.std()\n",
    "    # load adjacency matrix\n",
    "    ct_adj = pd.read_csv('data/CensusTract/adjacency_matrix_withweightDenver.csv', index_col=0)\n",
    "    ct_inorder = mapping_table.sort_values(by='node_id')['CT']\n",
    "    ct_adj = ct_adj.loc[ct_inorder, ct_inorder.astype(str)]\n",
    "    # min-max scale the weights\n",
    "    ct_adj = ct_adj / ct_adj.max().max() # min is 0\n",
    "    # fill nan with 0\n",
    "    ct_adj = ct_adj.fillna(0)\n",
    "    # load distance matrix\n",
    "    distm = pd.read_csv('data/OSRM/census_tract_trip_duration_matrix_Denver.csv', index_col='CT')\n",
    "    # define column order\n",
    "    cols = ['src', 'dst', 'count']\n",
    "    # return\n",
    "    data = {}\n",
    "    data['train'] = train[cols].values\n",
    "    data['valid'] = valid[cols].values\n",
    "    data['test'] = test[cols].values\n",
    "    data['train_inflow'] = inflow_train.values\n",
    "    data['train_outflow'] = outflow_train.values\n",
    "    data['num_nodes'] = ct_adj.shape[0]\n",
    "    data['node_feats'] = node_feats.values\n",
    "    data['ct_adjacency_withweight'] = ct_adj.values\n",
    "    data['distm'] = distm.values\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bd405631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ct2nid(dataframe, mapping_table):\n",
    "    '''\n",
    "    Mapping BoroCT2010 code to node id.\n",
    "\n",
    "    Inputs: \n",
    "    -----------------------------------\n",
    "    dataframe: The dataframe of trips. it is supposed to have 3 columns: h_BoroCT2010, w_BoroCT2010, count\n",
    "    mapping_table: The table of mapping between node id and BoroCT2010. The table is supposed to have two columns: node_id, BoroCT2010\n",
    "\n",
    "    Returns:\n",
    "    -----------------------------------\n",
    "    frame: it is supposed to have 3 columns: src, dst, count. src and dst are the node id of source and target respectively.\n",
    "    '''\n",
    "    frame = dataframe.copy()\n",
    "    mapping = mapping_table.copy()\n",
    "    # do the mapping\n",
    "    mapping = mapping.set_index('CT')\n",
    "    frame['src'] = mapping.loc[frame['h_geocode']].values\n",
    "    frame['dst'] = mapping.loc[frame['w_geocode']].values\n",
    "    # return\n",
    "    return frame[['src', 'dst', 'count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6c3dfbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_feat_from_srcemb_dstemb_dist(triplets, src_emb, dst_emb, dist):\n",
    "    feat_src = src_emb[triplets[:, 0]]\n",
    "    feat_dst = dst_emb[triplets[:, 1]]\n",
    "    feat_dist = dist[triplets[:, 0], triplets[:, 1]].reshape(-1, 1)\n",
    "    X = np.concatenate([feat_src, feat_dst, feat_dist], axis=1)\n",
    "    y = triplets[:, 2]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6ca00692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(y_hat, y):\n",
    "    '''\n",
    "    Root Mean Square Error Metric\n",
    "    '''\n",
    "    return np.sqrt(np.mean((y_hat - y)**2))\n",
    "\n",
    "def CPC(y_hat, y):\n",
    "    '''\n",
    "    Common Part of Commuters Metric\n",
    "    '''\n",
    "    common = np.min(np.stack((y_hat, y), axis=1), axis=1)\n",
    "    return 2 * np.sum(common) / (np.sum(y_hat) + np.sum(y))\n",
    "\n",
    "def CPL(y_hat, y):\n",
    "    '''\n",
    "    Common Part of Links Metric. \n",
    "    \n",
    "    Check the topology.\n",
    "    '''\n",
    "    yy_hat = y_hat > 0\n",
    "    yy = y > 0\n",
    "    return 2 * np.sum(yy_hat * yy) / (np.sum(yy_hat) + np.sum(yy))\n",
    "\n",
    "def MAPE(y_hat, y):\n",
    "    '''\n",
    "    Mean Absolute Percentage Error Metric\n",
    "    '''\n",
    "    abserror = np.abs(y_hat - y)\n",
    "    return np.mean(abserror / y)\n",
    "\n",
    "def MAE(y_hat, y):\n",
    "    '''\n",
    "    Mean Absolute Error Metric\n",
    "    '''\n",
    "    abserror = np.abs(y_hat - y)\n",
    "    return np.mean(abserror)\n",
    "\n",
    "def NRMSE(RMSE, y):\n",
    "    '''\n",
    "    Normalized Root Mean Square Error Metric\n",
    "    '''\n",
    "    return RMSE/(y.max() - y.min())\n",
    "\n",
    "def evaluate(y_hat, y):\n",
    "    '''\n",
    "    Evaluate the error in different metrics\n",
    "    '''\n",
    "    # metric\n",
    "    rmse = RMSE(y_hat, y)\n",
    "    mae = MAE(y_hat, y)\n",
    "    mape = MAPE(y_hat, y)\n",
    "    cpc = CPC(y_hat, y)\n",
    "    cpl = CPL(y_hat, y)\n",
    "    nrmse = NRMSE(rmse,y)\n",
    "    # return\n",
    "    return {'RMSE': rmse, 'MAE': mae, 'MAPE': mape, 'CPC': cpc, 'CPL': cpl, 'NRMSE':nrmse}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87afb90",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b4499251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading files\n",
    "cityA_features = pd.read_csv('data/PLUTO/columbus_ct_ratios2015.csv')\n",
    "cityB_features = pd.read_csv('data/PLUTO/denver_ct_ratios2015.csv')\n",
    "\n",
    "cityA_features = cityA_features[cityB_features.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8943553",
   "metadata": {},
   "source": [
    "#### Find nearest neighbors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e95897fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the nearest neighbors\n",
    "X_train = cityA_features.drop(columns=['CT'])\n",
    "y_train = cityA_features['CT']\n",
    "\n",
    "knn = NearestNeighbors()\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "X_test = cityB_features.drop(columns=['CT'])\n",
    "y_test = cityB_features['CT']\n",
    "\n",
    "list_ct, list_neighbors = [], []\n",
    "for ix in cityB_features.index:\n",
    "  list_ct.append(y_test[ix])\n",
    "  list_neighbors.append(knn.kneighbors(X_test.iloc[ix,:].values.reshape(1, -1))[1])\n",
    "\n",
    "data = {'ct_cityB': list_ct, 'neighbors_cityA': list_neighbors}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea35748",
   "metadata": {},
   "source": [
    "#### Preprocess dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "122373bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data = load_dataset()\n",
    "# parse dataset\n",
    "train_data = data['train']\n",
    "valid_data = data['valid']\n",
    "test_data = data['test']\n",
    "distm = data['distm']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7822613",
   "metadata": {},
   "source": [
    "#### Create new src and dst embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1520079d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings\n",
    "epath = 'embeddings/censustract_embeddings_year2015_layers1_emb128_multitask(0.5, 0.25, 0.25).npz'\n",
    "embeddings = np.load(epath, allow_pickle=True)\n",
    "# parse embeddings\n",
    "src_emb, dst_emb = embeddings['arr_0'], embeddings['arr_1']\n",
    "# scale distance matrix\n",
    "scaled_distm = distm / distm.max() * np.max([src_emb.max(), dst_emb.max()])\n",
    "# construct src and dst embeddings to city B from city A\n",
    "src_emb_new, dst_emb_new = [], []\n",
    "\n",
    "for neigh in df['neighbors_cityA']:\n",
    "    src_emb_new.append(src_emb[neigh[0]].mean(axis=0))\n",
    "    dst_emb_new.append(dst_emb[neigh[0]].mean(axis=0))\n",
    "\n",
    "src_emb_new = np.array(src_emb_new)\n",
    "src_emb_new = np.array(src_emb_new)\n",
    "\n",
    "# construct sample features\n",
    "X_train, y_train = construct_feat_from_srcemb_dstemb_dist(train_data, src_emb_new, src_emb_new, scaled_distm)\n",
    "X_valid, y_valid = construct_feat_from_srcemb_dstemb_dist(valid_data, src_emb_new, src_emb_new, scaled_distm)\n",
    "X_test, y_test = construct_feat_from_srcemb_dstemb_dist(test_data, src_emb_new, src_emb_new, scaled_distm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f075e43",
   "metadata": {},
   "source": [
    "#### Training model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c999ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train gbrt\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, random_state=2019, n_estimators=100)\n",
    "gbrt.fit(X_train, y_train)\n",
    "# test\n",
    "y_gbrt = gbrt.predict(X_test)\n",
    "res = evaluate(y_gbrt, y_test)\n",
    "# save result\n",
    "with open('outputs/overall_performance.csv', 'a+') as fout:\n",
    "    # create writer\n",
    "    cols = ['model','RMSE', 'MAE', 'MAPE', 'CPC', 'CPL','NRMSE']\n",
    "    writer = csv.DictWriter(fout, fieldnames=cols)\n",
    "    # create model name\n",
    "    model_name = 'GMEL_year2019_layers1_emb128_multitask(0.5, 0.25, 0.25)'\n",
    "    res['model'] = model_name\n",
    "    # write result\n",
    "    writer.writerow(res)\n",
    "# # save model\n",
    "# with open('models/gbrt_year2015_layers1_emb128_multitask(0.5, 0.25, 0.25).txt', 'a+') as fout:\n",
    "#     pickle.dump(gbrt, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8e26ecc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>CPC</th>\n",
       "      <th>CPL</th>\n",
       "      <th>NRMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GMEL_year2019_layers1_emb128_multitask(0.5, 0....</td>\n",
       "      <td>9.162269</td>\n",
       "      <td>4.646489</td>\n",
       "      <td>1.378245</td>\n",
       "      <td>0.670043</td>\n",
       "      <td>0.99964</td>\n",
       "      <td>0.062328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GMEL_year2019_layers1_emb128_multitask(0.5, 0....</td>\n",
       "      <td>8.648222</td>\n",
       "      <td>4.451674</td>\n",
       "      <td>1.305319</td>\n",
       "      <td>0.683602</td>\n",
       "      <td>0.99982</td>\n",
       "      <td>0.058831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model      RMSE       MAE  \\\n",
       "0  GMEL_year2019_layers1_emb128_multitask(0.5, 0....  9.162269  4.646489   \n",
       "1  GMEL_year2019_layers1_emb128_multitask(0.5, 0....  8.648222  4.451674   \n",
       "\n",
       "       MAPE       CPC      CPL     NRMSE  \n",
       "0  1.378245  0.670043  0.99964  0.062328  \n",
       "1  1.305319  0.683602  0.99982  0.058831  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv('outputs/overall_performance.csv', names=['model','RMSE', 'MAE', 'MAPE', 'CPC', 'CPL','NRMSE'])\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b779763d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78009918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
